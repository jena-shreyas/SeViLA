type 1
1
2
1
type 1
0
2
0
| distributed init (rank 1, world 2): env://
| distributed init (rank 0, world 2): env://
cdr2587:84918:84918 [0] NCCL INFO Bootstrap : Using ib0:172.19.146.24<0>
cdr2587:84918:84918 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
cdr2587:84918:84918 [0] NCCL INFO NET/IB : Using [0]hfi1_0:1/IB ; OOB ib0:172.19.146.24<0>
cdr2587:84918:84918 [0] NCCL INFO Using network IB
NCCL version 2.12.12+cuda11.7
cdr2587:84919:84919 [1] NCCL INFO Bootstrap : Using ib0:172.19.146.24<0>
cdr2587:84919:84919 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
cdr2587:84919:84919 [1] NCCL INFO NET/IB : Using [0]hfi1_0:1/IB ; OOB ib0:172.19.146.24<0>
cdr2587:84919:84919 [1] NCCL INFO Using network IB
cdr2587:84918:85030 [0] NCCL INFO Setting affinity for GPU 0 to 01
cdr2587:84918:85030 [0] NCCL INFO Channel 00/04 :    0   1
cdr2587:84918:85030 [0] NCCL INFO Channel 01/04 :    0   1
cdr2587:84918:85030 [0] NCCL INFO Channel 02/04 :    0   1
cdr2587:84918:85030 [0] NCCL INFO Channel 03/04 :    0   1
cdr2587:84918:85030 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
cdr2587:84919:85033 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0
cdr2587:84918:85030 [0] NCCL INFO Channel 00 : 0[18000] -> 1[86000] via P2P/IPC
cdr2587:84919:85033 [1] NCCL INFO Channel 00 : 1[86000] -> 0[18000] via P2P/IPC
cdr2587:84918:85030 [0] NCCL INFO Channel 01 : 0[18000] -> 1[86000] via P2P/IPC
cdr2587:84918:85030 [0] NCCL INFO Channel 02 : 0[18000] -> 1[86000] via P2P/IPC
cdr2587:84919:85033 [1] NCCL INFO Channel 01 : 1[86000] -> 0[18000] via P2P/IPC
cdr2587:84918:85030 [0] NCCL INFO Channel 03 : 0[18000] -> 1[86000] via P2P/IPC
cdr2587:84919:85033 [1] NCCL INFO Channel 02 : 1[86000] -> 0[18000] via P2P/IPC
cdr2587:84919:85033 [1] NCCL INFO Channel 03 : 1[86000] -> 0[18000] via P2P/IPC
cdr2587:84919:85033 [1] NCCL INFO Connected all rings
cdr2587:84919:85033 [1] NCCL INFO Connected all trees
cdr2587:84919:85033 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
cdr2587:84919:85033 [1] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer
cdr2587:84918:85030 [0] NCCL INFO Connected all rings
cdr2587:84918:85030 [0] NCCL INFO Connected all trees
cdr2587:84918:85030 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
cdr2587:84918:85030 [0] NCCL INFO 4 coll channels, 4 p2p channels, 4 p2p channels per peer
cdr2587:84919:85033 [1] NCCL INFO comm 0x2b266c0090d0 rank 1 nranks 2 cudaDev 1 busId 86000 - Init COMPLETE
cdr2587:84918:85030 [0] NCCL INFO comm 0x2af5240090d0 rank 0 nranks 2 cudaDev 0 busId 18000 - Init COMPLETE
cdr2587:84918:84918 [0] NCCL INFO Launch mode Parallel
Dataset config :  {'data_type': 'videos', 'build_info': {'annotations': {'train': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/train_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/train_tiny.json'}, 'val': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/val_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/val_tiny.json'}, 'test': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/test_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/test_tiny.json'}}, 'videos': {'storage': '/scratch/jenas/BTP/SeViLA/data/videos'}}, 'vis_processor': {'train': {'name': 'blip2_video_train', 'n_frms': 32, 'image_size': 224}, 'eval': {'name': 'blip_video_eval', 'n_frms': 32, 'image_size': 224}}, 'text_processor': {'train': {'name': 'blip_question', 'max_words': 50}, 'eval': {'name': 'blip_question', 'max_words': 50}}}
Config keys :  dict_keys(['data_type', 'build_info', 'vis_processor', 'text_processor'])
Config : 

 {'data_type': 'videos', 'build_info': {'annotations': {'train': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/train_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/train_tiny.json'}, 'val': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/val_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/val_tiny.json'}, 'test': {'url': '/scratch/jenas/BTP/SeViLA/data/annotations/test_tiny.json', 'storage': '/scratch/jenas/BTP/SeViLA/data/annotations/test_tiny.json'}}, 'videos': {'storage': '/scratch/jenas/BTP/SeViLA/data/videos'}}, 'vis_processor': {'train': {'name': 'blip2_video_train', 'n_frms': 32, 'image_size': 224}, 'eval': {'name': 'blip_video_eval', 'n_frms': 32, 'image_size': 224}}, 'text_processor': {'train': {'name': 'blip_question', 'max_words': 50}, 'eval': {'name': 'blip_question', 'max_words': 50}}}
Building datasets in base task ...
Inside build processors ...
Config name :  blip2_video_train
Config name :  blip_video_eval
Config name :  blip_question
Config name :  blip_question
in build() fn, Processors built
Run config :  {'task': 'videoqa', 'lr_sched': 'linear_warmup_cosine_lr', 'init_lr': 3e-05, 'min_lr': 0, 'warmup_lr': 1e-08, 'warmup_steps': 1000, 'weight_decay': 0.05, 'max_epoch': 5, 'batch_size_train': 1, 'batch_size_eval': 2, 'num_workers': 1, 'accum_grad_iters': 8, 'max_len': 30, 'min_len': 8, 'num_beams': 5, 'seed': 42, 'output_dir': 'expts/causalvidqa_ft/18_12_2023_Dec_41_29', 'amp': True, 'resume_ckpt_path': None, 'evaluate': False, 'train_splits': ['train'], 'valid_splits': ['val'], 'test_splits': ['test'], 'device': 'cuda', 'world_size': 2, 'dist_url': 'env://', 'distributed': True, 'find_unused_parameters': True, 'rank': 0, 'gpu': 0, 'dist_backend': 'nccl'}
Train: data epoch: [0]  [ 0/50]  eta: 0:14:25  lr: 0.000000  loss: 2.2255  time: 17.3192  data: 0.0000  max mem: 21490
Train: data epoch: [0]  [49/50]  eta: 0:00:04  lr: 0.000001  loss: 4.4433  time: 3.6583  data: 0.0000  max mem: 24479
Train: data epoch: [0] Total time: 0:03:26 (4.1252 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:04:36    time: 11.0490  data: 4.8099  max mem: 24479
Evaluation  [10/25]  eta: 0:01:08    time: 4.5856  data: 0.4444  max mem: 24479
Evaluation  [20/25]  eta: 0:00:22    time: 4.2233  data: 0.0086  max mem: 24479
Evaluation  [24/25]  eta: 0:00:04    time: 4.2455  data: 0.0303  max mem: 24479
Evaluation Total time: 0:01:50 (4.4322 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/val_epoch0.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [1]  [ 0/50]  eta: 0:08:19  lr: 0.000027  loss: 1.2237  time: 9.9804  data: 0.0000  max mem: 24479
Train: data epoch: [1]  [49/50]  eta: 0:00:04  lr: 0.000027  loss: 1.5416  time: 4.2652  data: 0.0000  max mem: 24479
Train: data epoch: [1] Total time: 0:03:26 (4.1350 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:04:04    time: 9.7763  data: 4.7930  max mem: 24479
Evaluation  [10/25]  eta: 0:01:02    time: 4.1709  data: 0.4517  max mem: 24479
Evaluation  [20/25]  eta: 0:00:19    time: 3.5326  data: 0.0174  max mem: 24479
Evaluation  [24/25]  eta: 0:00:03    time: 3.4439  data: 0.0358  max mem: 24479
Evaluation Total time: 0:01:33 (3.7589 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/val_epoch1.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [2]  [ 0/50]  eta: 0:06:15  lr: 0.000020  loss: 0.2779  time: 7.5098  data: 0.0000  max mem: 24479
Train: data epoch: [2]  [49/50]  eta: 0:00:04  lr: 0.000020  loss: 1.7738  time: 3.8862  data: 0.0000  max mem: 27451
Train: data epoch: [2] Total time: 0:03:46 (4.5333 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:04:06    time: 9.8422  data: 4.8670  max mem: 27451
Evaluation  [10/25]  eta: 0:01:01    time: 4.1102  data: 0.4574  max mem: 27451
Evaluation  [20/25]  eta: 0:00:18    time: 3.4584  data: 0.0167  max mem: 27451
Evaluation  [24/25]  eta: 0:00:03    time: 3.3707  data: 0.0344  max mem: 27451
Evaluation Total time: 0:01:32 (3.6951 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/val_epoch2.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [3]  [ 0/50]  eta: 0:09:11  lr: 0.000010  loss: 1.1488  time: 11.0394  data: 0.0000  max mem: 27451
Train: data epoch: [3]  [49/50]  eta: 0:00:04  lr: 0.000010  loss: 0.5874  time: 4.2011  data: 0.0000  max mem: 27451
Train: data epoch: [3] Total time: 0:03:34 (4.2941 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:04:03    time: 9.7203  data: 4.7340  max mem: 27451
Evaluation  [10/25]  eta: 0:01:00    time: 4.0615  data: 0.4439  max mem: 27451
Evaluation  [20/25]  eta: 0:00:18    time: 3.4658  data: 0.0127  max mem: 27451
Evaluation  [24/25]  eta: 0:00:03    time: 3.3935  data: 0.0354  max mem: 27451
Evaluation Total time: 0:01:32 (3.7095 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/val_epoch3.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [4]  [ 0/50]  eta: 0:12:19  lr: 0.000003  loss: 2.3099  time: 14.7854  data: 0.0000  max mem: 27451
Train: data epoch: [4]  [49/50]  eta: 0:00:04  lr: 0.000003  loss: 0.5387  time: 3.9475  data: 0.0000  max mem: 27451
Train: data epoch: [4] Total time: 0:03:25 (4.1041 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:04:06    time: 9.8600  data: 4.9006  max mem: 27451
Evaluation  [10/25]  eta: 0:01:01    time: 4.0826  data: 0.4577  max mem: 27451
Evaluation  [20/25]  eta: 0:00:18    time: 3.4749  data: 0.0129  max mem: 27451
Evaluation  [24/25]  eta: 0:00:03    time: 3.3876  data: 0.0340  max mem: 27451
Evaluation Total time: 0:01:32 (3.7110 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/val_epoch4.json
evaluate(); Test splits :  ['test']
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [ 0/25]  eta: 0:02:48    time: 6.7526  data: 2.2592  max mem: 27451
Evaluation  [10/25]  eta: 0:01:03    time: 4.2343  data: 0.2164  max mem: 27451
Evaluation  [20/25]  eta: 0:00:20    time: 3.9765  data: 0.0159  max mem: 27451
Evaluation  [24/25]  eta: 0:00:04    time: 3.7209  data: 0.0370  max mem: 27451
Evaluation Total time: 0:01:40 (4.0101 s / it)
result file saved to /scratch/jenas/BTP/SeViLA/lavis/expts/causalvidqa_ft/18_12_2023_Dec_41_29/result/test_epochbest.json
cdr2587:84918:84918 [0] NCCL INFO comm 0x2af5240090d0 rank 0 nranks 2 cudaDev 0 busId 18000 - Abort COMPLETE
cdr2587:84919:84919 [1] NCCL INFO comm 0x2b266c0090d0 rank 1 nranks 2 cudaDev 1 busId 86000 - Abort COMPLETE
